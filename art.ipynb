{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 validated image filenames belonging to 10 classes.\n",
      "Found 20 validated image filenames belonging to 10 classes.\n",
      "100\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "base_dir = 'data'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "def get_sampled_file_paths(directory, sample_fraction=0.002):\n",
    "    all_classes = os.listdir(directory)\n",
    "    sampled_file_paths = []\n",
    "    labels = []\n",
    "\n",
    "    for class_name in all_classes:\n",
    "        class_path = os.path.join(directory, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            all_files = os.listdir(class_path)\n",
    "            num_files_to_select = int(len(all_files) * sample_fraction)\n",
    "            sampled_files = np.random.choice(all_files, size=num_files_to_select, replace=False)\n",
    "            for file_name in sampled_files:\n",
    "                sampled_file_paths.append(os.path.join(class_name, file_name))\n",
    "                labels.append(class_name)\n",
    "\n",
    "    return sampled_file_paths, labels\n",
    "\n",
    "train_sampled_file_paths, train_labels = get_sampled_file_paths(train_dir)\n",
    "\n",
    "test_sampled_file_paths, test_labels = get_sampled_file_paths(test_dir)\n",
    "\n",
    "df_train = pd.DataFrame({\n",
    "    'filename': [os.path.join(train_dir, path) for path in train_sampled_file_paths],\n",
    "    'class': train_labels\n",
    "})\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    'filename': [os.path.join(test_dir, path) for path in test_sampled_file_paths],\n",
    "    'class': test_labels\n",
    "})\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    df_train,\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "    df_test,\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb'\n",
    ")\n",
    "\n",
    "print(train_generator.samples)\n",
    "print(validation_generator.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# Użycie pretrenowanego modelu VGG16 bez górnych warstw\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "# Zamrażamy wagi pretrenowanych warstw\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Dodanie własnych warstw |na wierzchu VGG16\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu', kernel_regularizer=l2(0.01)),  # Regularizacja L2\n",
    "    Dropout(0.5),\n",
    "    Dense(len(train_generator.class_indices), activation='softmax')\n",
    "])\n",
    "\n",
    "# Kompilacja modelu\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dawid/.pyenv/versions/3.11.8/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 12s/step - accuracy: 0.2063 - loss: 15.4969 - val_accuracy: 0.1000 - val_loss: 17.7159\n",
      "Epoch 2/50\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 15s/step - accuracy: 0.0625 - loss: 18.2155"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-19 20:08:20.307851: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "/Users/dawid/.pyenv/versions/3.11.8/lib/python3.11/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8s/step - accuracy: 0.0625 - loss: 18.2155 - val_accuracy: 0.1000 - val_loss: 14.2164\n",
      "Epoch 3/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 15s/step - accuracy: 0.1179 - loss: 16.9557 - val_accuracy: 0.1000 - val_loss: 9.9945\n",
      "Epoch 4/50\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 16s/step - accuracy: 0.0938 - loss: 13.3674"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-19 20:09:44.641494: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6s/step - accuracy: 0.0938 - loss: 13.3674 - val_accuracy: 0.1000 - val_loss: 8.6746\n",
      "Epoch 5/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 24s/step - accuracy: 0.1419 - loss: 12.3540 - val_accuracy: 0.1500 - val_loss: 8.8203\n",
      "Epoch 6/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5s/step - accuracy: 0.0000e+00 - loss: 13.2051 - val_accuracy: 0.1000 - val_loss: 9.1084\n",
      "Epoch 7/50\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3s/step - accuracy: 0.0000e+00 - loss: 9.4736"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 649ms/step - accuracy: 0.1000 - loss: 2.3034\n",
      "Test accuracy: 0.10\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(validation_generator, steps=validation_generator.samples // 32)\n",
    "print(f'Test accuracy: {test_acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "def predict_image(img_path, model):\n",
    "    img = image.load_img(img_path, target_size=(150, 150), color_mode='rgba')  # Obsługa plików PNG\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = np.argmax(predictions)\n",
    "    class_labels = list(train_generator.class_indices.keys())\n",
    "    return class_labels[predicted_class]\n",
    "\n",
    "img_path = 'data/train/sztuka nowoczesna/Screenshot 2024-07-25 at 01.22.33.png'\n",
    "predicted_class = predict_image(img_path, model)\n",
    "print(f'Ten obrazek przedstawia styl: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "import PIL.Image\n",
    "\n",
    "def on_upload_change(change):\n",
    "    img = PIL.Image.open(change['new'][0])\n",
    "    img_path = 'uploaded_image.png'\n",
    "    img.save(img_path)\n",
    "    display(img)\n",
    "    predicted_class = predict_image(img_path, model)\n",
    "    print(f'Ten obrazek przedstawia styl: {predicted_class}')\n",
    "\n",
    "upload_widget = widgets.FileUpload(accept='image/*', multiple=False)\n",
    "upload_widget.observe(on_upload_change, names='value')\n",
    "display(upload_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
